<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Types of Data · ml5js</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="There are many different types of data. You can have images, videos, text, sounds, files, graphs, and more. Here are some common types of datasets that might be interesting when using ml5js."/><meta property="og:title" content="Types of Data · ml5js"/><meta property="og:type" content="website"/><meta property="og:url" content="ml5js.org/index.html"/><meta property="og:description" content="There are many different types of data. You can have images, videos, text, sounds, files, graphs, and more. Here are some common types of datasets that might be interesting when using ml5js."/><meta property="og:image" content="ml5js.org/img/og.png"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="ml5js.org/img/og.png"/><link rel="shortcut icon" href="/img/favicon.png"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.css"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"/><script type="text/javascript" src="/scripts/p5.min.js"></script><script type="text/javascript" src="/scripts/p5.dom.min.js"></script><script type="text/javascript" src="/scripts/p5.sound.min.js"></script><script type="text/javascript" src="/scripts/ml5.min.js"></script><link rel="stylesheet" href="/css/main.css"/></head><body class="sideNavVisible doc separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/ml5.png" alt="ml5js"/></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class="siteNavGroupActive"><a href="/docs/getting-started" target="_self">Reference</a></li><li class="siteNavGroupActive"><a href="/docs/quick-start" target="_self">Examples</a></li><li class="siteNavGroupActive"><a href="/docs/data-overview" target="_self">Data</a></li><li class="siteNavGroupActive"><a href="/docs/training-introduction" target="_self">Training</a></li><li class=""><a href="/en/experiments" target="_self">Experiments</a></li><li class=""><a href="/docs/glossary-machine-learning" target="_self">Learn</a></li><li class=""><a href="https://github.com/ml5js" target="_self">Code</a></li><li class="navSearchWrapper reactNavSearchWrapper"><input type="text" id="search_input_react" placeholder="Search" title="Search"/></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><i></i></div><h2><i>›</i><span>Data</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup navGroupActive"><h3>Documentation</h3><ul><li class="navListItem"><a class="navItem" href="/docs/getting-started">Getting Started</a></li><li class="navListItem"><a class="navItem" href="/docs/ImageClassifier">imageClassifier()</a></li><li class="navListItem"><a class="navItem" href="/docs/FeatureExtractor">featureExtractor()</a></li><li class="navListItem"><a class="navItem" href="/docs/LSTMGenerator">LSTMGenerator()</a></li><li class="navListItem"><a class="navItem" href="/docs/PitchDetection">pitchDetection()</a></li><li class="navListItem"><a class="navItem" href="/docs/PoseNet">poseNet()</a></li><li class="navListItem"><a class="navItem" href="/docs/StyleTransfer">styleTransfer()</a></li><li class="navListItem"><a class="navItem" href="/docs/Word2vec">word2vec()</a></li><li class="navListItem"><a class="navItem" href="/docs/YOLO">YOLO()</a></li></ul></div><div class="navGroup navGroupActive"><h3>Examples</h3><ul><li class="navListItem"><a class="navItem" href="/docs/quick-start">Quick Start</a></li><li class="navListItem"><a class="navItem" href="/docs/image-classification-example">Image Classification</a></li><li class="navListItem"><a class="navItem" href="/docs/video-classification-example">Video Classification</a></li><li class="navListItem"><a class="navItem" href="/docs/custom-classifier">Classifier with Feature Extractor</a></li><li class="navListItem"><a class="navItem" href="/docs/custom-regression">Regression with Feature Extractor</a></li><li class="navListItem"><a class="navItem" href="/docs/lstm-example">Text Generation with LSTM</a></li><li class="navListItem"><a class="navItem" href="/docs/lstm-interactive-example">Interactive Text Generation LSTM</a></li><li class="navListItem"><a class="navItem" href="/docs/style-transfer-image-example">Style Transfer</a></li><li class="navListItem"><a class="navItem" href="/docs/style-transfer-webcam-example">Style Transfer with Webcam</a></li><li class="navListItem"><a class="navItem" href="/docs/posenet-webcam">PoseNet with Webcam</a></li><li class="navListItem"><a class="navItem" href="/docs/yolo-webcam">YOLO with Webcam</a></li><li class="navListItem"><a class="navItem" href="/docs/word2vec-example">Word2Vec</a></li></ul></div><div class="navGroup navGroupActive"><h3>Data</h3><ul><li class="navListItem"><a class="navItem" href="/docs/data-overview">Overview</a></li><li class="navListItem"><a class="navItem" href="/docs/data-collection">Data Collection</a></li><li class="navListItem navListItemActive"><a class="navItem navItemActive" href="/docs/data-types-of-datasets">Types of Data</a></li><li class="navListItem"><a class="navItem" href="/docs/data-preparing-your-own-data">Preparing your own data</a></li><li class="navListItem"><a class="navItem" href="/docs/data-creating-your-own-data">Creating your data</a></li></ul></div><div class="navGroup navGroupActive"><h3>Training</h3><ul><li class="navListItem"><a class="navItem" href="/docs/training-introduction">Introduction</a></li><li class="navListItem"><a class="navItem" href="/docs/training-setup">Setting up</a></li><li class="navListItem"><a class="navItem" href="/docs/training-lstm">Training a LSTM</a></li><li class="navListItem"><a class="navItem" href="/docs/training-styletransfer">Training Style Transfer</a></li><li class="navListItem"><a class="navItem" href="/docs/training-pix2pix">Training Pix2Pix</a></li></ul></div></div></section></div><script>
            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              const headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                if (event.target.tagName === 'A') {
                  document.body.classList.remove('tocActive');
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1>Types of Data</h1></header><article><div><span><p>There are many different types of data. You can have images, videos, text, sounds, files, graphs, and more. Here are some common types of datasets that might be interesting when using ml5js.</p>
<h2><a class="anchor" aria-hidden="true" id="images"></a><a href="#images" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Images</h2>
<p>Images often need to be resized to have consistent dimensions. Sometimes they will need to be converted to greyscale. It depends on the neural net architecture you are using so check the documentation to know for sure. One tool often used to process images programmatically is <a href="http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html">OpenCV</a>.</p>
<h2><a class="anchor" aria-hidden="true" id="text"></a><a href="#text" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Text</h2>
<p>There are many great resources online for gathering text datasets, such as <a href="https://www.gutenberg.org/">Project Gutenberg</a>, a collection of over fifty thousand free books with appropriate copyrights. Text that is scraped from other sources - such as a website, or a chat log - likely needs to be cleaned and processed. Processing can include tokenizing (splitting
text into words or sentences), removing punctuation, filtering out stopwords (words that you might not care about/don't want to process), and normalizing (such as making sure all the words have the same capitalization).</p>
<p>For more resources on dealing with text data, check out Dan Shiffman's <a href="http://shiffman.net/a2z/intro/">A2Z course</a>!</p>
<h2><a class="anchor" aria-hidden="true" id="sound-and-music"></a><a href="#sound-and-music" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Sound and Music</h2>
<p>Music is different from other mediums because it can be represented in many ways. One thing to think about when making a music dataset is how you want to represent your music. Possible machine learning music inputs include audio files (such as wav files), <a href="https://en.wikipedia.org/wiki/MIDI">MIDI</a>, <a href="https://en.wikipedia.org/wiki/Spectrogram">spectograms</a>/<a href="https://magenta.tensorflow.org/nsynth">rainbowgrams</a>, sheet music, raw audio, or ABC notation. Your music dataset is then a collection of wav files, midi files, ABC notation files, or spectograms. Each of these types of inputs work best with certain machine learning algorithms. Ror instance, text-based music notation often works with LSTMs, whereas spectograms often work best with the same algorithms employed for images.</p>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="data-collection">← Data Collection</a><a class="docs-next button" href="data-preparing-your-own-data">Preparing your own data →</a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#images">Images</a></li><li><a href="#text">Text</a></li><li><a href="#sound-and-music">Sound and Music</a></li></ul></nav></div><footer class="nav-footer" id="footer"><section class="sitemap"><div><h5>Docs</h5><a href="/docs/getting-started.html">Getting Started</a><a href="/docs/ImageClassifier.html">API Reference</a><a href="/docs/training-setup.html">Training Models</a></div><div><h5>Learning</h5><a href="/docs/tutorials.html">Tutorials</a><a href="/docs/glossary-machine-learning.html">Glossary</a><a href="/docs/resources.html">Resources</a></div><div><h5>Contribute</h5><a href="/experiments.html">Experiments</a><a href="https://github.com/ml5js/ml5-library/blob/master/CONTRIBUTING.md">Contributing</a><a class="github-button" href="https://github.com/ml5js/ml5-library" data-icon="octicon-star" data-count-href="https://github.com/ml5js/ml5-library" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></div></section><a href="https://itp.nyu.edu" target="_blank" class="fbOpenSource"><img src="/img/itp_logo.png" alt="Facebook Open Source" width="60" height="45"/></a><section class="copyright">This project is currently being maintained at NYU ITP by a community of teachers, residents and students.</section></footer></div><script type="text/javascript" src="https://cdn.jsdelivr.net/docsearch.js/1/docsearch.min.js"></script><script>window.twttr=(function(d,s, id){var js,fjs=d.getElementsByTagName(s)[0],t=window.twttr||{};if(d.getElementById(id))return t;js=d.createElement(s);js.id=id;js.src='https://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js, fjs);t._e = [];t.ready = function(f) {t._e.push(f);};return t;}(document, 'script', 'twitter-wjs'));</script><script>
              var search = docsearch({
                
                apiKey: '4e9582fa59998b865a9fd98ae8d8a9cc',
                indexName: 'ml5js',
                inputSelector: '#search_input_react'
              });
            </script></body></html>